
\section{Integrated Information Theory}

In this section, we present a summary of Integrated Information Theory as presented by Tononi et all \cite{main}.

\subsection{Definitions and Notation}
We take the term \textit{mechanism} to mean a non-empty set of nodes, connected in some way to each other, such that the current state of a node depends on the previous state of nodes which it is connected to. We show examples where nodes are AND, OR, XOR, and COPY gates. The \textit{order} of a mechanisms refers the number of nodes within the mechanism. A mechanism is called \textit{elementary} if it is of order one.


Let $G$ refer to a mechanism. We denote the current, past and future states of $G$ by $G^c$, $G^p$ and $G^f$ respectively. 

Suppose the current state, or part of it is known (i.e. we know the values of one or more nodes) and we call this information $X$. Then we define the \textit{cause repertoire} to be the distribution of past states that could have caused $G^c = X$. We abuse probability notation by writing this as: $p(G^p | G^c = X)$. Similarly the \textit{effect repertoire} is the distribution of future states which may have been caused by $G^c = X$. Again, we write this as $p(G^f|G^c=X)$. 

Finally, we define the \textit{unconstrained past repertoire} of a mechanism $G$ to be the distribution of states of $G^p$ with unconstrained outputs. In practice, this distribution is uniform. We denote this distribution by: $p^{uc}(G^p)$. The \textit{unconstrained future repertoire} is a distribution of future states of $G$ with unconstrained \textit{inputs}, similarly denoted by $p^{uc}(G^f)$. That the inputs instead of the outputs are constrained results in this distribution not being uniform.

\subsection{The earth mover's distance (EMD)}
In the context of this document, we will frequently calculate the distance between two probability distributions $p1$ and $p1$. For this we will use the mathematical notation $D(p1,p2)$, where $D$ is referring to the earth mover's distance.

To use this distance measure, we require the existence of another metric which measures the distance between any pair of states. Here we use the Hamming distance for this purpose, that is the number of individual bytes which disagree when comparing two states. (i.e. the hamming distance between 001 and 101 is 1, as the have 1 disagreement in the first digit)

In simple terms, if we think of each distribution as piles of dirt on top of each given state. The earth mover's distance is the product of how much dirt needs to moved by how far this dirt needs to go. 

\begin{description}
	\item[EMD between Probability Distributions] Let $S$ be a set of $n$ states, $\mathbf{D} = \left[ D_{ij}\right]$ where $D_{ij}$ is the distance between $s_i$ and $s_j$, $P$ and $Q$ be two probability distributions over $S$ such that $P(X = s_i) = p_i$ and $Q(X=s_j) = q_j$, $\forall i,j$. 
	
	Calculate $\mathbf{F} = \left[F_{ij}\right]$ matrix which optimises the following problem:
	
	\begin{align}
	\label{eq:EMD1}
	\min \limits_{\mathbf{F}}\sum \limits_{i,j=1}^n &F_{ij} D_{ij}\quad \text{subject to}\\
	&F_{ij}\geq 0,\quad \forall 1 \leq i,j \leq n\\
	\sum \limits_{j=1}^n &F_{ij} = p_i,\quad 1 \leq i \leq n\\
	\sum \limits_{i=1}^n &F_{ij} = q_j,\quad 1 \leq j \leq n
	\end{align}
	
	Finally, we say: 
	
	\begin{equation}
	\label{def:EMD}
	\text{EMD}(P, Q) = \sum \limits_{i,j=1}^{n} F_{ij} D_{ij}
	\end{equation}
\end{description}

From now on, we'll use the notation $D(P,Q)$ to denote the earth mover's distance instead of bothering with $\text{EMD}(P,Q)$ each time.

\subsection{Calculations for mechanisms}\label{sec:little_phi}

\subsubsection{cause-effect information}
The cause-effect information ($cei$) is a measure of the significance a piece of information has. It is calculated in 2 parts. First the cause information ($ci$) is defined by:

\begin{equation}
\label{def:ci}
ci(G^p|G^c = X) = D\left(p(G^p|G^c = X)||p^{uc}(G^p)\right)
\end{equation}


Next the effect information ($ei$) is defined as:
\begin{equation}
\label{def:ei}
ci(G^f|G^c = X) = D\left(p(G^f|G^c = X)||p^{uc}(G^f)\right)
\end{equation}

Finally, the $cei$ is calculated as:

\begin{equation}
\label{def:cei}
cei(G^c = X) = \min\left(ci(G^p|G^c = X), ei(G^f|G^c = X \right)
\end{equation}

Intuitively we understand the $ei$ as how the knowledge of $G^c = X$ effect our expectation for the future. Similarly $ci$ measures how much this present knowledge specifies the past. Together, the $cei$ measures how much information beyond the present does the piece of present information give.

\subsubsection{integrated information}
IIT assumes that only information which is irreducible can contribute to consciousness. This motivates the calculation of $\varphi$, which calculates the distance between the cause-effect repertoires of an entire mechanism, and the cause-effect repertoires of the partitioned mechanism. 

Intuitively, if a mechanism can be split into parts and these smaller mechanism generate the same result, then the mechanism is reducible and $\varphi=0$. In contrast, the larger $\varphi$ is, the more information the mechanism has beyond the sum of its parts.

Let $G$ be a mechanism, $X$ a state of $G$ such that $G^c = X$, and $P(G)$ be the power set of $G$; that is the set of all subsets of $G$. Let $N, M \in P(g)$, then we define the following notation:

\begin{equation}
\label{def:preMIP}
p(A|B /M, N)= p(M|N) p(A \backslash M |B \backslash N)
\end{equation}

Then

\begin{equation}
\label{def:phi2}
\varphi_{cause}(G^p|G^c=X/ M, N) = D \left( p(G^p|G^c=X) ,p(G^p|G^c=X/M,N)  \right)
\end{equation}.

With the benefit of this notation, we now introduce the notion of the minimum information partition (MIP). Specifically, the MIP refers to the sets $M'$ and $N'$ which minimise $\varphi_{cause}$. We then have:

\begin{equation}
\label{def:phi3}
\varphi^{\text{MIP}}_{cause}(G^p|G^c=X) = \varphi_{cause}(G^p|G^c=X/ M', N')
\end{equation}

$\varphi_{effect}$ is defined similarly. One need only swap the $p$ superscript with a $f$ superscript. 

Finally we can define integrated information $\varphi$:
\begin{equation}
\label{def:phi}
\varphi^{\text{MIP}}(G^{p,f}|G^c = X) = \min \left( \varphi^{\text{MIP}}_{cause}(G^p|G^c=X), \varphi^{\text{MIP}}_{effect}(G^f|G^c=X)  \right) 
\end{equation}



If $M'$ and $N'$ give rise to the cause MIP, we write the following:
\begin{equation}
\label{def:notation1}
G^c/G^p \rightarrow (M'/N') \times (G^c \backslash M'/ G^p\backslash N')
\end{equation}

\begin{description}
	\item[Note] In this section, we demonstrate how to find the MIP for $G^c/G^p$ or $G^c/G^f$. However, this calculation does not require us to be working with the entire mechanism $G$. We can also compute the MIP for $A^c/B^p, \quad \forall A,B \in P(G)$. Once we have the MIP, we can also compute $\varphi^{\text{MIP}}(B|A)$ as before.
\end{description}

\subsubsection{maximally integrated information}
It follows from the assumptions of IIT that a mechanism has a unique cause and effect. We identify these as the maximally irreducible causes and effects respectively. 

Let $G^c = X$. We now ask the question: what is the cause of $X$? Any element in $P(G)$ is a candidate, so we need to conduct an exhaustive search. For each $S \in P(G)$, we calculate $\varphi^{\text{MIP}}_{\text{cause}}(S^p|G^c=X)$. We call the set $S'$ which maximised $\varphi^{\text{MIP}}$ the \textit{core cause}, and define 
\begin{equation}
\label{def:core_cause}
\varphi^{\text{Max}}_{\text{cause}}(X):=\varphi^{\text{MIP}}_{\text{cause}}(G^p = S'^p|G^c = X)
\end{equation}

We identify the \textit{core effect} and calculate $\varphi^{\text{Max}}_{\text{effect}}$ in the equivalent fashion, and finish with setting $\varphi^{\text{Max}}$ to be the minimum of the two as before.

Let $S$ and $R$ be the sets which maximise $\varphi^{\text{MIP}}_{\text{cause}}(G^p = S|G^c = X)$ and $\varphi^{\text{MIP}}_{\text{effect}}(G^f = R|G^c = X)$ respectively. Then if $\varphi^{\text{Max}}(X)\neq 0$, we call $X/S^p,R^f$ a \textit{concept}. Given a state $X$, a concept has a maximally irreducible cause-effect repertoire (MICE). This refers to the combination of the maximally irreducible cause repertoire: $p(S^p|G^c=X)$ and the maximally irreducible effect repertoire $p(R^f|G^c=X)$.

\subsection{Calculations for systems of mechanisms}
Thus far calculations have been restricted to a single mechanism, whether 1st, 2nd, or $n$th order. However, a network of more than one node may contain multiply mechanisms, and these may share nodes. For example, a network of 3 node could potentially contain 7 mechanisms (all possible subsets of nodes less the empty set).

Now we turn our attention to a system of mechanisms, i.e. all mechanisms contained within a set of nodes. 

\subsubsection{conceptual information}
To calculate the conceptual information (CI) given a current state ($X$) for a system of mechanisms, first calculate $\varphi^{\text{Max}}(X)$ for each individual mechanism in the system as well as the corresponding MICE. Any mechanism for which $\varphi^{\text{Max}}(X)=0$ we neglect, as it contributed no conceptual information.

Denote $M_i$ for the MICE for the $i$th mechanism with a concept, and $\varphi^{\text{Max}}_i(X)$ for the integrate information of the $i$th concept. Then the conceptual information of a sate $X$ is given by:

\begin{equation}
\label{def:CI}
CI(X) = \sum \limits_i \varphi^{\text{Max}}_i(X) D(M_i, M^{uc})
\end{equation}

where $M^{uc}$ refers to the `null' concept, i.e. a combination of the unconstrained past and future repertoires. $D$ again refers to the earth mover's distance, although in a more generalised form from that originally 
described. 

Thus, the conceptual information of a system is affected by how many concepts exist within the system, and by how much information each of these concepts possess.
\subsubsection{integrated conceptual information}
Denoted by $\Phi$, integrated conceptual information is the value which we seek to compute.


\subsubsection{maximally irreducible conceptual structure}


\subsection{Computational cost}
Previously in section \ref{sec:little_phi}, we presented the functions $cei$, $\varphi$ and $\varphi^{MAX}$. Now we examine these more closely in or to estimate the cost of computing these functions. 

First, we note that each term of the form $p(G^p|G^c)$ is probability distribution over the states of $G$. We represent this as a vector of length $2^n$ (where $|G| = n$). The computational time required by the EMD is $O(N^3 \log N)$, where $N$ is the size of the distribution. Hence, computing $cei$ is $O(n2^{6n})$.

Next for computing $\varphi$ and $\varphi^{MAX}$, we make the assumption that no calculation is repeated. In other words, we have a perfect memory for what we have calculated. In the process to find $\varphi^{MAX}$, we will need to compute $p(A|B= X), \quad \forall A\in P(G^p), B\in P(G^c)$. Since $P(G^p)$ and $P(G^c)$ have a size $O(2^n)$, it follows that if no computations are repeated, $O(2^{2n})$ will be required. For each of these computations, a distance computation will also be required. Thus we have $O(n2^{8n})$. 

Finally, this computes $\varphi^{MAX}$ given one particular initial state (out of $2^n$). To compute $\varphi^{MAX}$ for all possible starting state, the cost would be $O(n2^{9n})$.

\section{Modelling an Organisation}

In this chapter, we seek to model an organisation which seeks to maximise its profits. In particular, we interest ourselves in the case where revenue is gained by the completion of `projects' which are assigned at random points in time. The organisation has a set of teams at its disposal, to whom it assigns each project as it appears. We assume the following factors which influence how we assign teams to projects:

\begin{enumerate}
	\item Each project requires a particular set of `skills' in order to be completed. 
	
	\item Each team possesses one or more of these `skills'. 
	
	\item A project can be completed only when sufficiently many teams are assigned to it that possess all of the `skills' required by the project.


\end{enumerate}

Within these assumptions, there are many variations of models we can build. This follows from the different ways we can choose the following:

\begin{enumerate}
	\item Progression of time. We take our model to be a discrete iterative process. Each iteration marks the passage of a fixed time period. 

	\item Number of `skill' types. This value we select in advance and is constant.
	\item Number of teams. We take this to be constant for each version of our model.

	\item Number of Projects. We assume the likelihood that a project will require one particular skill to be independent of whether or requires any other skill. Hence we take the arrival of Projects to be based on a multivariate Bernoulli distribution. 
	
	\item How projects are completed. We arbitrarily decide that the duration of a project will be distributed exponentially. This is equivalent to saying that each project has an identical chance of completion following each iteration.
	

\end{enumerate}


\subsection{A single-cluster model}
In this model, we consider there to be 3 teams, and 2 skill types. We label the teams as $A$, $B$ and $C$, and the skill types as $\alpha$ and $\beta$. We take the assignment of skills to teams as fixed, deciding arbitrarily that teams $A$ and $C$ have skill $\alpha$, while teams $B$ and $C$ have skill $\beta$. This arrangement is illustrated in figure \ref{fig:diagram1}.

\begin{figure}[h]
	\centering

	\includegraphics[]{ModelDiagram.png}
	\caption{We have 3 types of nodes: skill types ($\alpha$ and $\beta$), teams ($A$, $B$ and $C$), and incoming project alerts ($P_\alpha$ and $P_\beta$). If a team node is joined to a skill node by an edge, then the team possesses that skill. All of the teams have one way connections with the incoming project alerts nodes because the teams respective state do not effect incoming projects' state, but are effected by them.}
	\label{fig:diagram1}
\end{figure}

With the teams, skills and assignment of these defined, all that remains to be specified for this model is the following:

\begin{enumerate}
	\item The probabilities affecting arrival of projects.
	\item The process by which projects are assigned to teams.
	\item The probabilities affecting completion of projects.
\end{enumerate}

\begin{steps}
	
	\item \textbf{Arrival of projects}\\
	Earlier we stated that this process was based on a multivariate Bernoulli distribution. Specifically we use the 2-dimensional version, as we are working with 2 types of skills. We specify the vector of probabilities: $\mathbf{p} = (p_\alpha, p_\beta)$ such that an incoming project requires skill $\alpha$ with probability $p_\alpha$ and similarly for skill $\beta$.
	
	We create 2 `incoming project alert' nodes which we label as $P_\alpha$ and $P_\beta$. Thee state of these nodes is determined only by the previously mentioned probability distribution. Hence these nodes transition states independently of any other node. 
	
	\item \textbf{Assignment of projects}\\
	First are the 3 simple cases. If a project which requires skill $\alpha$ but not $\beta$ arrives, and team $A$ is idle, then the project is assigned to $A$, resulting in team $A$ turning on. Similarly teams $B$ and $C$ will turn on if a project arrives tailored to their abilities while they are not already active.	
	
	2 more cases exist: If a project suitable for team $A$ arrives while $A$ is busy and $C$ is idle, then $C$ will be assigned $A$'s task. The same will apply if $B$ is busy when such a project arrives. Finally, if a project requiring both skills arrives while $C$ is busy while $A$ and $B$ are both idle, then $A$ and $B$ will be assigned the project, resulting in both of them transitioning to a state of `on'.
	
	\item \textbf{Completion of projects}\\
	We already stated the time required to complete a project to be modelled by th exponential distribution. This was so we could use a simple Bernoulli process to determine when a project is completed. This means that the probability of completing a project is constant w.r.t. time. However, to make this process more realistic, we decide that the probability of completing a project will not be independent of active working connections.
	
	We define the parameters $k_A$, $k_B$ and $k_C$ to represent the effectiveness of each team. If $A$ is active, then it will complete its task with probability $k_A$ after each time-step, and so on. 
	
	However, when teams are working on similar tasks, then collaboration occurs. Specifically, if teams $A$ and $C$ are both active, since skill $\alpha$ is required for both projects, $C$ will assist $A$. However, as $C$'s project also involved skill $\beta$, $A$ will be unable to return the assistance. In this case, $A$ will finish its task after the next time-step with probability $k_A + k_B$, which for $C$ things are unchanged. 
	
	$C$ can only receive assistance when all teams are active, since together $A$ and $B$ account for all required skills. In this final case, $C$ will complete its task with probability $k_A+k_B+k_C$.
\end{steps}


Now we can put this all together to form the state by node transition probability matrix: $\mathbf{T}$, defined as:
\begin{equation}
\mathbf{T}_{ij}= P(G_j^{t+1}=1 | G^t = i)
\end{equation}
where $G$ refers to the entire network with $G_j$ standing for $A$, $B$ and $C$ for $j=1,2,3$. $\mathbf{T}$ will have a row for each state of the system, and a column for each node. Hence its shape will be $2^n \times n$ assuming that each node has exactly 2 states (as in IIT). We present the matrix in table \ref{mat:model1}.


\begin{table}[h]
	\centering
	

	$\begin{array}{l|ccc}
	\hline
	\multicolumn{1}{|l|}{\text{State}} & \multicolumn{1}{c|}{A} & \multicolumn{1}{c|}{B} & \multicolumn{1}{c|}{C} \\ \hline
	000                         & .25                    & .25                    & .25                    \\
	100                         & 1-k_A                  & .25                    & .5                     \\
	010                         & .25                    & 1-k_B                  & .5                     \\
	110                         & 1-k_A                  & 1-k_B                  & .75                    \\
	001                         & .5                     & .5                     & 1-k_C                  \\
	101                         & 1-k_A-k_C              & .25                    & 1-k_C                  \\
	011                         & .25                    & 1-k_B-k_C              & 1-k_C                  \\
	111                         & 1-k_A-k_C              & 1-k_B-k_C              & 1-k_A-k_B-k_C         
	\end{array}$
	\caption{The state by node transition probability matrix for the model.}
	\label{mat:model1}
\end{table}

\begin{description}
	\item[Note] You may notice that this matrix refers to a 3 node system, while the network presented in figure \ref{fig:diagram1} contains 5 nodes which switch states. This is because the incoming project alert nodes switch `on' and `off' randomly, and independently of anything else. To simplify the IIT computation, we average over all possible external inputs. This results in reducing the $32\times5$ matrix to a $8\times3$ matrix.
	
	To do this, we `noise' out the nodes $P_{\alpha}$ and $P_{\beta}$, which involves taking $p_A$ and $p_B$ to both be 50\%. In other words, the valeus $p_A$ and $p_B$ have no effect on the calculation of $\Phi$, as they are merely external inputs, and not integrated into the dynamics of $A$, $B$ and $C$. With this assumption made, we average over all possible states of $P_\alpha$ and $P_\beta$ to gain the reduced matrix.
\end{description}

Finally, for this model to be interesting, we need to have some control over the parameters. For this, we think of $k_A$, $k_B$ and $k_C$ of being directly related to the funding invested in each team. Hence we assume $k_A+k_B+k_C = K$, where $K$ is a constant which refers to the total amount of funding at the organisations disposal. $k_i$'s refer to how we allocate that funding.

When solving for this model, we will employ two methods: IIT and a classical approach. For IIT, we will compute the value of $\Phi$, while for the classical approach, we instead will introduce the notion of costs, and from this compute the economic performance with respect to the parameters $k_A$, $k_B$ and $k_C$. 

\subsubsection{Classical Approach to solving the model}
Currently in our model a project will definitely be completed, the question is how long it will take. Thus we don't have to worry about the cost of failed projects. Instead, we concern ourselves with the value of missed projects: those which arrive when there is no suitable team or combination of team available to be assigned to it. Define $W_A$, $W_B$ and $W_C$ to be the waste incurred when a project requiring skills $\alpha$, $\beta$ and both  is missed respectively.

When does this waste happen? Look back at table \ref{mat:model1}. Based on how we've define the allocation process, the only states at which waste can occur are: $101$, $011$ and $111$. If the state is $101$, then wast occurs if a project which requires skill $\alpha$ arrives. If skill $\beta$ is further required, then the cost is $W_C$, else the cost is $W_A$. Since each combination of skills can arrive with equal probability (assumed for the sake of IIT), the expected loss when the state is $101$ is $0.25(W_A+W_C)$.
Similarly the costs for states $011$ and $111$ are $0.25(W_B+W_C)$ and $0.25(W_A+W_B+W_C)$ respectively.

Now we know how much money we expect to waste for each state. Now the question is: how often do we expect to be at each state? We can take two approaches: either we can simulate the system and compute waste from the time series, or else we can compute the expected waste per iteration by finding the stationary distribution of the system. We take the latter approach.

When the values of $k_i \in (0,1)$, then the $8\times8$ state by state transition probability matrix $\mathbf{M}$ will have an eigenvalue of 1 with geometric multiplicity 1. Furthermore, $-1$ will not be an eigenvalue. Hence the left eigenvector with eigenvalue 1 will the be stationary distribution of the system, which precisely describes the long term behaviour of the system. 

Hence our cost function is:
\begin{equation}
\label{eq:simple_model_cost_function}
f(k_A, k_B, k_C) = \mathbf{q} \cdot \mathbf{v}(\mathbf{M}(k_A, k_B, k_C))
\end{equation}

where $\mathbf{v}$ is the normalised eigenvector corresponding to the eigenvalue 1 of the matrix $\mathbf{M}$ which in turn depends on $k_A$, $k_B$ and $k_C$. $\mathbf{eta}$ is a vector which contains the cost of each state. It can be written as: 
\begin{equation}
q_i = 0.25(W_A+W_C)\delta(i-5) + 0.25(W_B+W_C)\delta(i-6) + 0.25(W_A+W_B+W_C)\delta(i-7)
\end{equation}

In short, given the constants $W_A$, $W_B$ and $W_C$, we express the expected waste of the system as a function of $k_A$, $k_B$ and $k_C$. 

\subsubsection{Significance of $\Phi$}

In this section we compare the results of IIT with the expected waste of the network. $\Phi$ depends on the matrix $\mathbf{M}$ and the initial state. meanwhile $f$ depends on $\mathbf{M}$ in addition to the constants $W_A$, $W_B$, and $W_C$. The independence of $\Phi$ and waste constants is alarming, and leaves little optimism regarding any correlation. 

Since $k_A + k+B+ k+C = K$, our parameter space is 2-D. We therefore compute $\Phi$ and $f$ for all suitable values of $k_A$,\ldots, on a grid with points at a distance 0.01 from each other. We use 2 initial states for the purpose of computing $\Phi$ which are arbitrarily taken to be $000$ and $101$.

\begin{figure}[h!]
	\centering
	\includegraphics[scale = 0.5]{PhiCostcomp1.png}
	\caption{We plot the value of $\Phi$ given an initial state of $000$  against the cost of missed projects. The value of $K$ is 0.5, and $W_A$, $W_B$ and $W_C$ are 0.4, 0.4 and 0.2 respectively. }
	\label{fig:PhiCostcomp1}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale = 0.5]{PhiCostcomp2.png}
	\caption{We plot the value of $\Phi$ given an initial state of $101$ against the cost of missed projects. The value of $K$ is 0.5, and $W_A$, $W_B$ and $W_C$ are 0.4, 0.4 and 0.2 respectively. }
	\label{fig:PhiCostcomp2}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale = 0.5]{Phi2Phicomp.png}
	\caption{We plot the value of $\Phi$ given an initial state of $101$ against $Phi$ given the initial state $000$. The value of $K$ is 0.5, and $W_A$, $W_B$ and $W_C$ are 0.4, 0.4 and 0.2 respectively. }
	\label{fig:Ph2Phicomp}
\end{figure}

In figures \ref{fig:PhiCostcomp1}, \ref{fig:PhiCostcomp2} and \ref{fig:Ph2Phicomp}, we plot the values of $f$ computed against those of $\Phi$ given each of the 2 initial states. In particular we see in figure \ref{fig:PhiCostcomp1} that while there seems to be some form of relationship between the cost of wasted projects and the `consciousness', whatever relationship that exists does not appear to be particularly useful. 

Similarly in figure \ref{fig:PhiCostcomp2} it seems like there might be something interesting happening in a dimension which we are not measuring. However, we can't gain insight as to what that is with these measurements. Finally when we compare the values of $\Phi$ for different initial states in figure \ref{fig:Ph2Phicomp}, we find that the two values are almost entirely independent of each other. 

In general, $\Phi$ does not provide much insight into this model. Why might this be? We present 2 potential flaws of the model:

\begin{enumerate}
	\item What $\Phi$ computes is the difference in information that a system has, above an beyond what it would have if the system was partitioned. As things stand, our model is a fully connected 3 node system. Thus we can't expect interesting $\Phi$ behaviour, as there are no partitions which might radically affect our network. 

	\item The mechanism we defined for allocation of projects is one which assumes perfect knowledge and communication. We never have a scenario in which two teams work on the same thing unnecessarily, or one team neglects working on a project becomes it assumes that another team is. These events are far more indicative of waste, then what was measured previously. 

\end{enumerate}

Motivated by these observations, we proceed to the next model.



\subsection{A two cluster model}

The purpose of this next model is to deliberately allow scenarios where realistic miscommunication might occur which results in waste. The question of interest is whether $\Phi$ can measure such miscommunication.

We extend the previous model as follows: 
\begin{enumerate}
	\item Now we take there to be 3 skills: $\alpha, \beta$ and $\gamma$.
	
	\item As before we have nodes $A$, $B$ and $C$ which interact in the same way as previously. In addition, we include the nodes $D$ and $E$. $D$ is equipped with skills $\alpha$ and $\gamma$, while $E$ has skills $\beta$ and $\gamma$. 
\end{enumerate}

We illustrate the new system in the following figure \ref{fig:diagram2}.
\begin{figure}[h]
	\centering	
	\includegraphics[]{Model2Diagram.png}
	\caption{We have 3 types of nodes: skill types ($\alpha$, $\beta$, and $\gamma$), teams ($A$, $B$, $C$, $D$ and $E$), and incoming project alerts ($P_\alpha$, $P_\beta$ and $P_\gamma$). If a team node is joined to a skill node by an edge, then the team possesses that skill. The teams receive inputs from the incoming project alert nodes, but the dependence is strictly one directional.
	Certain teams are connected, indicating that they are actively communicating with each other so to distribute work efficiently. Note that we have 2 disconnected clusters of teams. This will give rise to waste.}
	\label{fig:diagram2}
\end{figure}

In the previous model, we constructed the TPM from a global perspective. This was valid since all the teams involved were in communication. For this model, I need to construct the decision process for each team regarding when to take up a project independently. This will allow for easier adjustment to my TPM when I add an edge joining the two clusters. For this purpose, we introduce the following notation:

Let $\{G^t\}_{t=0}$ be a time series referring to the networks states. For a particular point in time $t_0$, we have $G^{t_0} = (A^{t_0}, B^{t_0}, C^{t_0}, D^{t_0}, E^{t_0})$, where each $A^{i}$ is a binary number; that is the node $A$ is either on or off. Similarly for the other nodes. We will use conventional boolean arithmetic notation ($+$ means `or', $\times$, or equivalent multiplication notation means `and' while $\overline{A}$ means `not $A$'.) Finally denote by $S(p)$ a random sample which if less than $p$ returns a 1, and otherwise 0.

With this notation, our old system can be written as follows:
\begin{align}
\begin{split}
\label{eq:transition_old}
A^{i+1} &= \overline{A}^{i} P_\alpha \left(1-P_\beta + P_\beta \overline{B^i} C^i\right)+ A^i \left(\overline{C}^i\overline{S(k_A)} + C^i \overline{S(k_A+k_C)}\right)\\
B^{i+1} &= \overline{B}^{i} P_\beta \left(1-P_\alpha + P_\alpha \overline{A^i} C^i\right)+ B^i \left(\overline{C}^i\overline{S(k_B)} + C^i \overline{S(k_B+k_C)}\right)\\
C^{i+1}&= \overline{C}^i\left(A^i P_\alpha(1-P_\beta) + B^i P_\beta(1-P_\alpha) + P_\alpha P_\beta \right) + C^i\left(\overline{A}^i\overline{B}^i\overline{S(k_C)} +A^i B^i \overline{S(k_A+k_B+k_C)}\right)
\end{split}
\end{align}

This system of difference equations is sensitive to the connectivity matrix, assuming that all three `team' nodes are connected to each other, and that all of these nodes see both $P_\alpha$ and $P_\beta$. In order to extend this to a more general system, we introduce additional notation. 

Before, $A^{i+1} = f(A^i, B^i)$ would mean that the future state of $A$ depends on the current states of $A$ and $B$. Now, we redefine the notation $B^i$ such that $B^i$ refers to the state of $B$ provided that $A$ is connected to $B$ and can see what that state is. In rigorous notation, we have redefined: $B^i := M_{a,b}B^i$, and $\overline{B}^i :=M_{a,b}\overline{B}^i$, where $\mathbf{M}$ is the connectivity matrix.

For convenience, we further define $B^i_*:= M_{a,b}M_{b,a}B^i$ and $\overline{B}^i_* = M_{a,b}M_{a,b}\overline{B}^i$. This refers to A being able to see $B$, and vice versa. 

\begin{align}
\begin{split}
\label{eq:general_transition_averaged}
\mathbf{T}_{.,1} &= \frac{1}{8}\overline{A}^{i} \left(1 + (1-\overline{C}^i )(\overline{B}^i_*+B^i\overline{E}^i_*)+ (1-\overline{D}^i)\overline{E}^i_* + \overline{E}^i\right)
\end{split}
\end{align}


Note that even now, node $A$, $B$ and $C$ do not receive any inputs from $P_\gamma$. That is, they do not perceive all available information, which will result in waste. 

On the other hand, $D$ and $E$ perceive all inputs, but not the state of other teams. Before we write the equations for $D^{i+1}$ and $E^{i+1}$, it is necessary to assume without loss of generality that the expected return from a project of requiring only skill $\alpha$ is greater than that for a project requiring skill $\beta$. Otherwise, if one arrives which requires only skill $\gamma$ while both $D$ and $E$ are idle, which should take the task? Having made this assumption however, it follows that $E$ should take the task, as it more likely to be beneficial keeping $D$'s skill set in reserve.

\begin{align}
\begin{split}
\label{eq:transition_new}
D^{i+1} &= \overline{D}^i \left( P_\gamma (1-P_\alpha)E^i + P_\alpha \right) + D^i\overline{S(k_D)}\\
E^{i+1} &= \overline{E}^i \left( P_\gamma + P_\beta-P_\beta P_\gamma \right) + E^i\overline{S(k_E)}
\end{split}
\end{align}

As before, we average over all possible inputs from $P_\alpha$, $P_\beta$ and $P_\gamma$ to reduce the TPM's size. This results in equations \ref{eq:transition_old} and \ref{eq:transition_new} becoming:

\begin{align}
\begin{split}
\label{eq:transition_averaged}
P(A^{i+1}=1) &= \frac{1}{4}\overline{A}^{i} \left(1 + \overline{B^i} C^i\right)+ A^i \left(\overline{C}^i(1-k_A) + C^i (1-k_A-k_C)\right)\\
P(B^{i+1}=1) &= \frac{1}{4}\overline{B}^{i} \left(1 + \overline{A^i} C^i\right)+ B^i \left(\overline{C}^i(1-k_B) + C^i (1-k_B-k_C)\right)\\
P(C^{i+1}=1)&= \frac{1}{4}\overline{C}^i\left(A^i  + B^i + 1 \right) + C^i \left(\overline{A}^i\overline{B}^i(1-k_C) +A^i B^i (1-k_A-k_B-k_C)\right)\\
P(D^{i+1}=1) &= \frac{1}{4}\overline{D}^i \left(2 +E^i \right) + D^i(1-k_D)\\
P(E^{i+1}=1) &= \frac{3}{4}\overline{E}^i  + E^i(1-k_E)\\
\end{split}
\end{align}

We construct the $32\times5$ TPM directly from equation \ref{eq:transition_averaged}, although we do not present it here due to its size.




\subsubsection{Solving the 2 cluster model}
First of all, this system is easy to solve from the perspective of IIT. One finds that there is a single complex: $ABC$. This complex has the same value of $\Phi$ given a state as previously. This means that $D$ and $E$ contain no integrated information as is. This is not surprising given that the difference equation for $E$ depends only on $E$, resulting in it being independent of $D$.


\subsection{Next steps}

Here we present additional features which might develop this model into being more useful:

\begin{enumerate}
	\item A continuum of states for a project. Before a project was either `on' or `off'. It would be better if 1 were to refer to unstarted, 0 referred to completed, and any number in between indicated the progress made. This would allow the completion rate of projects to not be memoryless.
	\item A more sophisticated mechanism for collaboration. 
\end{enumerate}



\section{Applying IIT}
In this section I intend to talk about observing how $\Phi$ actually behaves when I use it to measure consciousness. What type of shape it has, what type of optimal values can be achieved in practice. This section should include a detailed look at simple cases.

\subsection{2 nodes}
Examine how $\Phi$ behaves on networks with 2 nodes. As this case is small, try to conduct exhaustive searches in pursuit of some intuition as to how $\Phi$ behaves.
\subsection{3 nodes}
Attempt to generalise ideas from previous section here. Can I find an optimum with manageable computational effort? Can I apply principles from the 2 nodes case to anticipate what an optimum might be?

\subsection{Alternative Measures of $\Phi$}
Compare measurements of $\Phi$ using IIT 3.0 with results using alternative measures which have been proposed. Are they correlated. Are they measuring the same thing?

\section{Applicability of IIT to organisations}
In this section we consider the properties of $\Phi$, and question whether or not they are desirable properties in the context of a model for an organisation.

\begin{enumerate}
	\item \textbf{State Dependence}: Not only does the computation of $\Phi$ depend on the networks structure and transition mechanism, but it also depends on the state of every node within the system at the current time-step, but also on the previous state of every node outside the subsystem of consideration. This makes sense from the perspective of neuroscience as the level of consciousness is determined by which nodes are active at once, and not merely on the structure of the brain. Hence, within a fixed brain, consciousness various throughout the day. 
	
	In the context of a model for an organisation, a single time-step is not of particular interest if ones goal is simply to optimize profits. 
	
	
	\item \textbf{Consciousness of Deterministic Systems}: It is stated by Tegmark \cite{improvedIIT} that it is undesirable for a measurement of consciousness to be 0 for deterministic systems. Thus from the perspective of IIT, it is not necessarily unreasonable that numerical experiments would find $\Phi$ tending to be larger for deterministic systems than probabilistic ones.
	
	From the perspective of an organisation, a fully random decision process is obviously bad. However, 
\end{enumerate}